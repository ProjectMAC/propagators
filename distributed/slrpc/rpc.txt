                A Scheme Remote Procedure Call System

To enable a community of agents to collaborate on a task they must
have a way to communicate.  An RPC system is one approach to build
such a communication system.  We needed such a system for several
purposes: to make a distributed propagator network, and to enable the
construction of a server for some educational software components to
be used in a bigger system.

In this document I describe a particular RPC system that I have
constructed for use in Scheme programs.  I show what it can do, how it
can be used, why I made the particular design choices in my system,
and what future steps I am planning, including a detailed discussion
of the meaning of network errors in the presence of full
continuations.


                            What can it do

Although my RPC system can pass first-class functions, capture and
invoke continuations non-locally and re-entrantly, and support such
LISP-isms uninterned symbols and improper lists, I will first show an
elementary example.  Later we will see how these more sophisticated
features are implemented and how they can be used.

In my RPC system an agent can construct a server.  Another agent,
perhaps on a distant machine, construct a client for that server.
The server can execute procedures that are referred to by the client.
The server provides such services by registering a service procedure 
that the client can call with arguments.


                         Setting up a server

For example, a program can create a RPC server by incanting:

   (define *scmserver* (create-rpc-server))

The server is started, by 

   (start-rpc-server *scmserver* <port> <address>)

where <port> is a tcp port on the machine it resides on.  The
<address> is an IP address for that machine.  The thread that the
construced the server can be suspended using

   (block-indefinitely)

Each requsts to the server will be run in a new thread.


The server can be made to provide clients with the procedure
func-procedure with the name func by registering the procedure under
that name:

   (register-rpc-procedure *scmserver* "func" func-procedure)



                     Attaching and using a client

An agent, perhaps running on another machine, can make a client for
this server.  First the client is constructed

  (define *client* (create-rpc-client))

then the client is attached to the appropriate port on the server
machine

  (connect-rpc-client *client* <port> <address>)

The client can use the service provided by the server by:

  ((bind-rpc-call *client* "func") <arg1> ... <argn>)

which will return the value of the server procedure func-procedure  
applied to the server objects indicated by the arguments provided.

The client can be disconnected from the server by executing

  (disconnect-rpc-client *client*)



                          How it can be used

Here is a complete example of how this can be set up to provide remote
evaluation of expressions (a dangerous service!), protected with
a crude password system and separate environments for different users.

We set up a server on one machine.  Here is the appropriate user code:

(define *scmserver* (create-rpc-server))

(define server-port 1729)
(define key "\\dÛy{;\211Ò´\214[\030í\222u\v")

; id symbol, password hash, procedure
(define *scmserver-service-table*
  `(
    (new-env   ,key
           ,(lambda ()
          (extend-top-level-environment
              user-initial-environment)))
    (eval      ,key ,eval)
    ))

(define (scmserver-accessor requested-thing-id password)
  (let ((thing-record (assoc requested-thing-id *scmserver-service-table*)))
    (if (not thing-record)
    (error 'unknown-service-id requested-thing-id)
    (let* ((hash (md5-string password)))
      (if (string=? hash (cadr thing-record))
          (caddr thing-record)
          (begin
        (sleep-current-thread 1000)
        (error 'unauthorized)))))))


(register-rpc-procedure *scmserver* "access-thing" scmserver-accessor)

(define (server-start address port)
  (start-rpc-server *scmserver* port address)
  (write-line "Started rpc server")
  (block-indefinitely))

(define (server-stop)
  (stop-rpc-server *scmserver*))


And here is a matching client:

(define server-port 1729)

(define (client-connect host port)
  (connect-rpc-client client port address)
  client))

(define (client-disconnect client)
  (disconnect-rpc-client client))

(define (client-access-service client service-id password)
  ((bind-rpc-call client "access-thing")
   service-id password))


A sample use is:

;;; Assume that the password is "bar".

(define client (client-connect "127.0.0.1" server-port))

(define reval (client-access-service client 'eval "bar"))

(define env ((client-access-service client 'new-env "bar")))

(reval '(cons 'a 'b) env)
;Value: (a . b)

(reval '(define foo 3) env)
;Value: foo

(reval 'foo env)
;Value: 3

(define env1 ((client-access-service client 'new-env "bar")))
;Value: env1

(reval 'foo env)
;Value: 3

(reval 'foo env1)
;;; Error reported

(reval '(define foo 4) env1)
;Value: foo

(reval 'foo env)
;Value: 3

(reval 'foo env1)
;Value: 4

(client-disconnect client)



                      More complex capabilities

Notice that the above example passes closed procedures over the
network, fetched out of *scmserver-service-table*.  They become remote
proxy procedures on the receiving side, which, when invoked, trigger
the execution of the original procedures on the server side.  The
client can similarly pass procedures to the server that the server can
execute on the client.

Procedures passed over then network can even be continuations produced
by call-with-current-continuation.  In addition to their usual uses,
continuations add very interesting possibilities to remote procedure
call.  For example, they can be used to implement a message-passing
protocol without incurring the extra round-trip delays typically found
in RPC systems.  A client A can invoke a procedure on server B, which
can invoke a procedure on server C, and C can call an escape
continuation passed to it from A, without needing to return control to
B.  (Tail calls from machine to machine are not (yet) implemented,
although the same functionality can be achieved with continuations.)

Although eq?-equality is not preserved in general, it is preserved for
procedures, opaque "remote objects" (e.g. environments), and
uninterned symbols.  It is possible to support eq? more generally,
with a hash-cons like mechanism, but I decided that the overhead of
using it by default was probably not justified.  How to cleanly
specify the need for such a thing only where necessary remains an open
question.



                             How it works

The core of the RPC system is a simple S-expression passing protocol.
A procedure is invoked by packaging up a procedure identifier, the
argument list, and some metadata into an S-expression and sending it
over the wire.  A procedure returns a value in the same manner, by
invoking a particular designated argument as a return continuation.
If a procedure calls a different continuation than its designated
return continuation, so be it.  This forms the basis for full support
of continuations and message-like behavior.

However, some important data types (including closed procedures and
continuations) cannot faithfully be rendered as printable expressions.
Also, eq?-ness is completely un-representable in S-expressions. So, we
need some means to extend S-expressions.  Fortunately, a robust
mechanism to do so already exists: quasiquotation.  To construct
non-printable objects, we provide a set of special primitives for
their construction, which may be invoked from an unquote expression.
To establish their eq?-ness, we provide a primitive for binding an
internal identifier to a value, such that every time that value is
passed through the RPC system, it is branded with the same identifier
over the wire and is translated back to the same object in local
memory.  If an identifier has never been seen before on a particular
host, an appropriate object is constructed from scratch, either an
actual instance (in the case of value types like uninterned symbols)
or a proxy object (for host-attached objects like functions and record
types).

In order to prevent these object-passing features from leaking memory,
distributed garbage collection is needed.  This is fairly complicated
in the general case.  Luckily, for our purposes, we can make do with a
simpler distributed reference-counting scheme, since pointer cycles
spanning multiple machines do not arise frequently (and not at all in
purely functional code).  Each host in the system keeps a table of
peers to whom it has passed a reference to an object.  An object is
never garbage collected locally so long as any peers have outstanding
references.  Similarly, each host keeps a table of peers from whom it
has received such a reference.  When all local references are dropped
and the object is locally garbage collected, these peers are notified
in a message that the reference has been released.  Additionally, when
a session times out due to a failure, all references it was given are
automatically released.

The complicated, discontinuous dynamic extents allowed by arbitrary
continuations raise some additional issues that must be addressed.  In
the absence of non-local continuations, ordinary recursive invocation
of the RPC handlers would execute procedures within the appropriate
dynamic extents, providing, in particular, the correct values of
fluid-bound variables.  However, continuations allow extents to be
entered and exited fairly arbitrarily.  For example, a server
processing a client's call could re-enter a continuation captured from
a previous call and then invoke a callback on the client.  The client
must be informed that the callback should be executed in the extent of
this older continuation, in order that it see the correct values of
the client's fluid variables.  This is accomplished by packaging
metadata with calls identifying a continuation associated with the
current extent for each machine traversed by the call stack.

This metadata is packaged using a "mobile fluid binding" mechanism,
which ships around a dictionary of identifier/value pairs with every
call and fluid binds them locally before invoking procedures.  This
mechanism is also available to the programmer through the procedures
mobile-fluid-bind and mobile-fluid-query.  Mobile fluids are exposed
as a special lexical environment rather than as automatically bound,
ordinary identifiers because it is not immediately clear how to relate
lexical scopes on one machine to those on another.

(Mobile fluids also intentionally lack a set! interface. While it is
possible, in principle, to carry back updated fluid values to machines
higher on the call stack, this runs into complications with arbitrary
continuations.  For this reason, I intend to expose a "mobile thread
local storage" mechanism for mobile, mutable values like sequence
numbers, random number generator seeds, and debug reports.)

A facility not currently supported is a mechanism to construct
functions remotely.  For performance and robustness reasons, it can be
desirable to execute a block of code directly on a remote host,
proxying in and out once, rather than each time remote data is
accessed.  However, the RPC system does not currently provide any
means for constructing such mobile code.  The principal reason is that
it is not clear how to enforce referential transparency.  In
principal, one could expose a function combinator library, but it
would be necessary to collect remote versions of all dependencies,
even primitives like car, for otherwise, what's to say that the local
car is equivalent to the remote function bound to the same identifier?

Fundamentally, this is a tacit acknowledgment of the version skew
problem that plagues real distributed systems, for which we have no
immediate solutions.  Mandating that a particular set of primitives be
provided by the protocol, though possibly pragmatic, simply punts the
runtime environment's versioning problem to the protocol.  In
practice, a standard combinator mechanism might be useful in some
simple cases but probably would be too awkward for anything
complicated, and in any case could be easily rolled together by the
programmer.  A more sophisticated mechanism might be useful, but it
also might end up no more effective than asking the programmer to
export eval (in some appropriate environment) from their RPC servers.
The status quo, requiring all bootstrapping to pass through a table of
registered, programmer-specified names does not solve the versioning
problem in any sense, but it does make the default interface very
narrow, and it places the problem front and center in the programmer's
control, where they can easily add secondary dispatch mechanisms if
desired.



             Unfinished business: network error handling

What of the traditional Achilles' heel of RPC, network errors?  The
usual response of RPC systems is to leave the problem to the
programmer.  The RPC system only guarantees that a procedure
invocation will be executed at most one (if even that) and leaves it
to the programmer to retry or roll back or abort as appropriate.  This
can be very challenging.

Unfortunately, use of tail calls and continuations in RPC dramatically
expands the armada of partial failure scenarios that must be
considered. In traditional RPC, one needs to worry that a call that
errors out may have never executed, may have partially executed but
then failed partway (e.g. crash), may still be executing (possibly
still issuing recursive calls out on the network), or may have run to
completion but failed to successfully return the result. It is a
straightforward matter to discard any (functional) results returned
after a time-out has been declared, rendering the last two cases
fairly similar.

With continuations (or tail calls), one also needs to worry that a
call that has *not* clearly errored out may have failed, because
responsibility for it was quietly handed off to another machine that
did fail, even if the original callee did not.  Perhaps this could be
solved by application-level timeouts or by requiring callees to
continue to track the fates of calls they've handed away.  But, it
gets worse.

If a caller tires of waiting on the network and times out, returning
an error, but connectivity is re-established and the call turns out to
return successfully, what happens?  In traditional RPC, we can drop
the successful return based on its sequence number, because calls only
return once, and this one already returned an error.  With full
continuations, however, calls can return more than once.  Even if a
program never uses first-class continuations, the RPC system must be
prepared for the possibility that it might, so it is no longer so
simple to correctly ignore the second return.  This particular case
can be solved by invalidating the return continuation when a network
error is declared (breaking the possibility of reentrant returns when
connectivity fails, which is probably acceptable), which at least
gives us traditional RPC error semantics for programs that don't use
continuations.  For programs that do, however, the problem persists.

Consider the following scenario:
Host A invokes a procedure on host B. 
Host B recursively invokes a procedure on host C.
Host C calls an escape continuation on host A. 
Host C crashes.

What happens on host B?  Eventually, it recognizes that its connection
to C has timed out, and it returns an error to the user procedure it
was running.  What will this procedure do, now?  Who knows!  It might
retry and succeed, it might return an error, or it might do something
else.  In other words, in the presence of escape continuations, it's
possible that even a call which returned successfully could still be
executing somewhere out on the network!  Moreover, it is no longer a
simple matter to filter out and drop the duplicate returns, because
duplicate returns are perfectly legal with reentrant continuations.  A
second successful return from B could lead to duplicate execution at
the original call site and thereafter. The network error has
effectively split our thread into two.



                       Bifurcations and extent

Fundamentally, a remote call is a grant of control.  If, while
blocking on a remote call, a host decides that for whatever reason it
wants control back, the only reliable action is to wrest control by
fiat, i.e., to bifurcate the thread.  Any other option requires
coordinating with the current owner of the head of control, and hence,
waiting on the network.  Declaring a network error is one obvious
example where this is needed, but other notable cases exist, such as
aborting on a ^C signal.  So, we must acknowledge that bifurcations
are inevitable, even in traditional RPC.  The question becomes, what
do we do about them?  Traditional RPC automatically kills the original
branch of any bifurcation as soon as that branch gets around to
returning.  Continution-based RPC can be made to do the same in
limited cases (corresponding to programs with traditional control
flow) by invalidating the return continuation during a bifurcation,
but when programs exploit first-class continuations, fully automatic
killing isn't obviously feasible.

A related issue we have glossed over so far is the question of how
long it is appropriate for a continuation that blocked on the network
to continue monitoring for network errors.  Clearly, if the last
living reference to a continuation is a local one solely for the
purpose of network monitoring, the monitoring can stop and the
continuation can be garbage collected.  This, however, is gratuitously
conservative.  Just because a continuation still has outstanding
references does not mean that the programmer wishes the program to be
sensitive to whether the associated network connection is still alive.
The continuation may be an artifact, never to be invoked, or it may be
around solely for debugging purposes.  Monitoring for network errors
would produce unnecessary bifurcations with strange throwbacks in
control flow.

The obvious answer is that a continuation should monitor for network
errors during and only during its discontinuous extent.  Then, errors
are raised if and only if the network fails in such a way that the
presently active call stack is severed.  This is possible to
implement, but at a high cost.  Whenever a continuation is called, we
could perform a "synchronous dynamic wind", handing off control step
by step through every machine along the state space path traversed.
This would, in a sense, transform the continuation-based code into
continuation-passing code managed by a traditional RPC protocol.
Although continuation programming semantics are preserved, the
desirable performance properties are completely lost.  Tail calls must
trampoline back through the remote caller.  Escape continuations must
walk back up the stack like an ordinary return.  Coroutines become
ridiculous, with arbitrarily worse latency.  Surely we can do better.

There is indeed something fishy about this scheme.  Consider the case
of coroutines, where control is repeatedly handed back and forth
between two families of continuations, possibly deeply buried in their
own call stacks.  Logically, this is a local interaction and need only
concern the little corner of the network involved.  Why should
continuations higher up on the stack, possibly far across the network,
care about which of the two coroutines is active at any precise
moment?  Indeed, by the relativity of simultineity entailed in the
natural latency of network communication, it's not even well defined
to claim that events at one corner of the network are simultaneous
with events occuring at some distant site, unless you force
synchronous communication between those separated locales.  This
suggests that perhaps control transfers between continuations should
be reported up the call stack asynchronously, akin to the radiation of
waves in relativity.  If the communication reporting a nonlocal
control transfer has not yet arrived at a given point in the network,
it simply does not matter which call stack monitors for errors;
either, or both, will do.  The only crucial consideration is that at
least one stack is monitoring at all times and that it will detect any
network failure that prevents control hand-offs from being reported.

The notion of extent also interacts with the phenomenon of
bifurcation.  Two forks of a bifurcation may come to have similar call
stacks, with overlapping extents, or very dissimilar call stacks, with
concurrent and distinct extents.  Such complications will need to be
tolerated.



       Coming soon: Bifurcation tracking and async dynamic wind

With all this new complexity, leaving network error handling entirely
up to the programmer seems like a dubious design choice.  How might we
simplify the programmer's task?

One approach we might take is to try to expose bifurcations as
first-class entities.  In order to do this, we need a way to name
them.  In particular, we would like to capture the natural branching
structure of bifurcations within their naming scheme, so that we can
determine how two bifurcations relate without needing to coordinate
over the network (whose failure may be the reason for the existence of
the bifurcations).  A simple sequence number will not be adequate,
because bifurcations are partially ordered, not totally ordered: for
distinct bifurcations A and B, A may be an ancestor of B (representing
a point in B's past history, A < B), B may be an ancestor of A (B <
A), or A and B may represent independent branches descending from a
bifurcation that occured at common ancestor C (A <> B).

Instead of individual sequence numbers, one can identify bifurcations
by "sequence chains".  A sequence chain is an indentifier kept in
mobile thread-local storage, consisting of a historical list of
sequence numbers.  Every time control of a thread is transferred over
the network, its head sequence number is incremented.  Whenever
control is reclaimed and a bifurcation declared, a fresh sequence
number is pushed onto the head of the list.  That way, each sequence
number following the head documents a historical bifurcation.
Sequence chain A precedes B if the tail of A is a proper suffix of B
and if the head of A is less than or equal to the immediately
preceding element of B (which may be the head; if it is B's head and
is exactly equal, then A and B are equal).  If A neither precedes nor
follows B, then A and B are incomparable, parallel branches.

Note that the size of a sequence chain grows only with the size of a
sequence number in the absence of network errors or other bifurcations
(i.e. logarithmically), but in the presence of bifurcations, it grows
linearly in the number of bifurcations.  This should not present a
burden for short-lived threads or fairly reliable networks, but
long-lived threads in extremely flaky networks may need some
additional mechanism to periodically reset or garbage collect their
sequence chains.

Now that we have a scheme for naming bifurcations, we can offer the
programmer a simple abstraction to help with error handling in a very
common case: when a (sub)computation begins and ends on single machine
but may bounce arbitrarily around the network during processing.  More
generally, if a computation passes through a "choke-point" on a single
machine, we can avoid all the complications of distributed consensus
in determining what branch of a partitioned computation succeeds; we
simply pick the first one to arrive. (Of course, just as in
traditional RPC, the programmer must be careful about the side effects
of a computation; multiple attempts must be, in a sense, idempotent.)

The most obvious place to track the arrival of bifurcations is as they
depart some designated extent, i.e., the extent of some complicated
call that may experience errors.  We would like to declare the first
to finish as the winner.  Identifying the "first" bifurcation to exit
an extent is nontrivial in the presence of reentrant continuations,
however, because multiple exits and re-entries are be perfectly legal,
and there may be additional bifurcation events along the way.  But, we
can enforce that bifurcations represent monotonically non-decreasing
sequence chains.  Any exiting bifurcation that is a successor of the
last one to exit represents a reentrant return.  Any arrival that is
not a successor must represent the losing end of a bifurcation.  We
call the abstraction of a delimited extent that quietly kills any
bifurcations attempting to leave the extent with a non-monotonic
seqence chain a "turnstile".  In many cases, the programmer can simply
wrap complicated, cross-machine calls in a turnstile in order to
prevent internal bifurcations from escaping.  (Turnstiles cannot be
added automatically, however, because it is important that they not be
skipped over by calling an escape continuation; the programmer must
take this into account.)

Now, we still haven't addressed the issue of how long a continuation
blocked on the network should monitor and report connectivity
failures.  It must continue to do so at least until both control has
been transferred elsewhere, no longer recursively beneath it in the
stack, and the responsibility for monitoring for failures has been
handed off somewhere more proximate to the new head of control.  Once
these responsibilities have been handed off, there is no additional
need for the continuation to monitor locally.  With a synchronous
dynamic wind, these two responsibilities are tied together, handed off
up the stack simultaneously.  As discussed above, this comes at the
cost of a heavy latency penalty for the head of control.  However, we
can construct an "asynchronous dynamic wind" mechanism which splits
apart the two responsibilities, propagating the time-critical head of
control quickly while allowing failure monitoring to percolate at its
own pace.

The straw man scheme proposed earlier, leaving all blocked
continuations monitoring indefinitely until garbage collected, does
not suffice.  Although it splits apart the two responsibilities, it
never really informs unwound continuations of their fate; it doesn't
propagate failure monitoring so much as spray it around and forget.
In addition to the possibility of raising confusingly out-of-date
errors (largely a qualitative annoyance), it does not report errors
using updated mobile thread local storage, relying on stale values
stored with stale continuations.  The resulting outdated sequence
chains can trigger inappropriate bifurcation kills at turnstiles.  So,
we need a real dynamic winding scheme, one that is aware of the state
space path and can propagate information along it.

In the most general scenario of a continuation call, a machine A holds
the latest common ancestor stack frame shared between the original
continuation and the new continuation, a caller with the original
continuation is executing on a machine B, and the new continuation is
waiting to be called on a machine C.  At a high level, a continuation
call using asynchronous dynamic wind goes through the following steps:

 1. B calls the continuation on C as though it were an ordinary
 procedure call, handing off control and blocking for a reply or a
 network error.

 2. C receievs the call and recognizes it as a continuation.  It
 determines the state space path being traversed by comparing
 information in mobile fluid state between the old and new
 continuations.  It packages this path up into an argument and passes
 it off in an asynchronous call to the special async-dynamic-unwind
 procedure on B.

 3. Without waiting for a response, C calls the new continuation,
 performing any local dynamic-wind actions, and procedes on with the
 computation.

 4. Meanwhile, B receives the async-dynamic-unwind call and observes
 that a successor to its old continuation waiting blocked on the
 network has requested an unwind.  It locally unwinds that
 continuation to the preceding stub entry point, executing any local
 dynamic-wind actions, and recursively propagates the
 async-dynamic-unwind call to the next machine on the state space
 path.

 5. Unwinding continues propagaing machine by machine until it reaches
 a machine where the blocked continuation is its successor and not the
 other way around (i.e. another asynchronous dynamic wind happened
 from the other direction and out-raced the original one), in which
 case the wind completes early, or until it reaches machine A and the
 last common ancestor stack frame.

 6. At the last common ancestor frame, the re-wind phase begins. A
 identifies the continuation to be reactivated from the state space
 path and calls the special async-dynamic-rewind procedure,
 reactivating the continuation, updating its sequence chain, and
 executing any local dynamic-wind actions.  It then recursively
 calls async-dynamic-rewind on the next machine of the state space
 path.

 7. async-dynamic-rewind propagates much like async-dynamic-unwind
 before, but traveling down the stack along the new path, reactivating
 continuations and re-blocking them on the network.  It also completes
 early if it encounters a continuation whose sequence chain is a
 successor (either due to another asynchronous dynamic wind or an
 ordinary return).

 8. Eventually, if it does not complete early, the rewind reaches C
 and the end of the state space path.  It returns, and the process is
 complete.

If a network error occurs, propagation of the asynchronous dynamic
wind may end up stuck on the wrong side of a partition.  However, the
invariant is maintained that there will always be a continuation
monitoring the network upwards on the stack from any possible
partition.  It will either be from the stack of the original
continuation, not yet unwound, or from the new continuation, freshly
re-wound.  Additionally, the rewind process ensures that the freshest
available sequence chains will be used in raising errors, necessary
for proper reentrant operation of turnstiles.

Note that, in the absence of network errors, all expected dynamic-wind
actions are executed, though not necessarily in the traditional order
among multiple machines.  I do not expect this to be a burden in
ordinary use cases, but there may be some scenarios where dynamic-wind
is used to produce globally visible side effects for which the
inter-machine ordering matters.

So, we now have a mechanism that allows low-latency, non-local control
transfer over the network, yet also provides precise handling of
network errors.  Moreover, it does not depend on an end-to-end error
handling scheme, with poor cross-layer visibility and strange time-out
properties relying on difficult to calibrate parameters.  Instead, it
provides composable error handling.



                      Sloppy unfinished business

Besides not yet supporting first-class bifurcations and asynchronous
dynamic wind, the current codebase still has a variety of stupid loose
ends that need fixing.  Of particular note are issues that crop up
when transferring control among three or more machines.  The current
codebase assumes one thread per session, and so calling from machine
to machine requires hopping from session to session, necessitating
cross-thread calls.  Unfortunately, these are blocking calls, meaning
that it's possible to deadlock the RPC library by calling around a
cycle of machines.  The code needs to be reworked to treat client
threads as first-class entities, rather than trying to multiplex their
execution onto per-session threads.

Similarly, objects are forwarded from machine to machine in a rather
dumb way: the proxy objects are proxied, instead of creating new,
first-order proxies pointing to the object's original host.  This has
the unfortunate consequence that to actually call remote continuations
in the manner described above, without forcing the protocol to
synchronously wind through your call stack, you need to transfer the
continuation directly from creator to caller, and not through an
intermediate machine.  This ruins the whole design pattern of calling
escape continuations several machines deep into the stack.  Proxy
objects need to be packaged with metadata about the object's original
host, such that if a session connecting directly to that host is
available, the proxy operates over that session and not through the
path it arrived on.  (It may also be worth adding a means to fetch a
direct object reference, to improve the durability of the object in
the face of garbage collection after a machine failure along the path
the object traveled.)

Also, the way oridinary Scheme error conditions are proxied is
complete crap. I need to decide on something better.



                             Related work

Seamless but failure-intolerant distributed Schemes:
Kali Scheme (TOPLS Sept '95), Dreme, The Tube, etc...


An end-to-end systems approach for composing RPC calls without round
trips; has some similar capabilities, but is not nearly as composable or
general:
RPC Chains (NSDI '09)



