\documentclass[12pt]{article}

\begin{document}

\end{document}

Merge as a new kind of plumbing

Mergeable information is ``Improving'' values per burton-1991, but on
a lattice instead of a total order
- merge is join; where's the meet? do I need it? is this really a semilattice?
  - a stream of data can be fold join'd to produce a stream of monotonic data
- is there a functional implementation? what's the moral equivalent
  of the content operation (which seems rather disfunctional)
  - maybe not -- ``content'' could just be the head of the list
- the moral equivalent of his minimum operation seems to be a decider
  that waits until the content of a cell implies some question
  (or is completely determined) and then acts.

Correspondences:
- Improving foo <-> type-foo output of a propagator
  (where in my world foo is supposed to be partial information about a
  yet different bar)
- Maximum <-> merge (the join in the semi-lattice by implication of
  information)

Differences:
- Statically typed vs dynamically typed world
- single-threaded vs propagation execution model (maybe the loeb trick
  can fix this)
  - Improving a -> Improving a -> Improving a merging two streams
- functional vs imperative implementation
  - my content operation sounds strange, because its output changes
- minimum operation is a decider (I think) that says:
  if the information in the cell implies the information I have in my hand,
  I can stop looking at that cell and say ``yes''.  Otherwise I can't.


Does sigfpe's loeb trick (where did he get it?) offer a functional
implementation of propagation networks?  If not, why not?
- How can I dynamically construct more propagators?
  (This may actually be possible with streams that get forced
   to expand into more streams)


What about TMSes?  Adding premises to an information type seems not to
preserve the lattice property I need; taking lists of supported values
seems to restore it.  Is there a general mechanism of taking lists of
poset elements to turn them into the desired lattice structure?


Hm: Being a lattice homomorphism implies being a poset homomorphism.
So preserving the lattice structure is a stronger condition than
preserving the ordering structure.  In particular, the topological
sort homomorphism destroys potentially valuable lattice structure.
This applies to the mapping from the world into the program
representation.  Therefore, programming languages should support
posets and lattices directly.  There should be explicit join and meet
operations where appropriate, and <=> should return four possible
values: less than, equal, greater than, and incomparable.  (And
``sort'' should mean ``(stable) topological sort'', except that
topological sort is linear in the number of *edges*, so the reduction
to the nlogn case for data that are actually totally ordered might be
a pain to implement without type-like knowledge that tells you about
that situation.)

A consequence of this is that <=> really can map pointwise over
compound structures, producing the incomparable result in the
appropriate cases.  This is important because it allows meet and join
to map pointwise.  And, separately, it may be useful to be able to
refine an order by specifying the ordering of (some) pairs of elements
that the original order left incomparable.  This amounts to an
injective poset homomorphism that now doesn't preserve meet and join,
but may nonetheless be valuable.  The lexicographic ordering is an
example.

There are products in the category of lattices, but no products in the
category of totally ordered sets.  We want tupling to be a product.
This is why tupling and total order make a poor combination.

Pieces of my world:

Bounds-On foo
is a stream of foos that mean that the Platonically True foo is the
join of all of them.  Order of elements doesn't matter.
This is all I require the output of a propagator to look like.
Such a stream ending is a disconnect signal (which don't appear in my
world; since I can have nontrivial strongly-connected components in my
signalling graphs, I need a nonlocal process to detect quiescence).

merge :: Bounds-On foo -> Bounds-On foo -> Bounds-On foo
is critical.  It must mesh the two streams, picking elements from
whichever one has them available as they arise.  I don't know how to
implement this in straight Haskell.
- Gerry says: Peter Henderson?  Clinger 1981, Unbounded nondeterminism

drop-subsumed-terms :: Bounds-On foo -> Bounds-On foo
can be a form of garbage collection, but see below.

Improving foo
is a Bounds-On foo that is guaranteed to be strictly increasing (so
only the last, best element need be kept).  This is what the content
of a cell looks like.

strictify :: Bounds-On foo -> Improving foo
trivially carries the full join.  My cells are strictify compose merge

fmap :: (foo -> bar) -> Bounds-On foo -> Bounds-On bar
For f to commute with dropping subsumed terms, it must conserve the
order relation.  For f to commute with strictify, it must conserve
joins as well.

Bounds-On Bounds-On foo -> Bounds-On foo
is easy: just diagonalize (but need to use the merge function)

foo -> Bounds-On foo
is easy too (if fact, there are two of them: either [x] or [x x x x x
  x x ...]. The constant propagators in my network are effectively the
latter, but the former may be better for some purposes)

apply :: Bounds-On (foo -> bar) -> Bounds-On foo -> Bounds-On bar
(this is what is takes to make an Applicative Functor in Haskell)
Since Bounds-On is a monad, this is redundant:
Start with a Simpler object:
apply-1 :: Bounds-On (foo -> bar) -> foo -> Bounds-On bar
implemented by mapping; but the lattice on (foo -> bar) must be
compatible with the lattice on bar for each fixed foo.
(lambda (x bounded-f)
  (fmap (lambda (f) (f x)) bounded-f))
gives
foo -> Bounds-On (foo -> bar) -> Bounds-On bar
lift gives
Bounds-On foo -> Bounds-On Bounds-On (foo -> bar) -> Bounds-On Bounds-On bar
flatten gives
Bounds-On foo -> Bounds-On (foo -> bar) -> Bounds-On bar
The condition here is that
(lambda (f) (f x))
has to be a lattice homomorphism from functions to their ranges
for each x.  One way to do that is to define
(join f g) = (lambda (x) (join (f x) (g x)))
which provides the appropriate semilattice structure on functions.

If the semilattice structure on functions is 
(join f g) = (lambda (x) (join (f x) (g x)))
then we have a story for partially known functions:
just push the partialness into the function's return value.
In particular, the completely unknown function must be the identity
for this join; but note that (const b_min) *is* an identity for this
join.  Also therefore, a TMS over functions must just be a function
that returns a TMS over values (maybe I already said that above).
This, in turn, produces an answer to the cons conundrum (see below).

Bounds-On a -> Bounds-On b -> Bounds-On (a,b)
is easy; but we want the lattice structure on (a,b) to be a *lattice*
product of a and b, which the lexicographic order is not.
We also want the a and b lattices to have a lower bound, because
we want to map a to (a,b_min) and b to (a_min,b)

implied? :: foo -> Improving foo -> Bool
is a safe decision procedure.  It can return True before the Improving
quiesces, if the foo it has is already implied.


The cons conundrum:

The argument that convinces me goes as follows.  In the types, we
have:

(list a) = (pair a (list a)) | nil

which can be represented as

(list a) = (tuple pair? a (list a)),

with the invariant that if pair? is false, I promise not to look in
the a and (list a) slots of the tuple (so they can be blank to avoid
infinite regress, but that possibility need not be admitted).  Now,
this can be TMSed:

(tms (list a)) = (tms (tuple pair? a (list a)))
               = (tuple (tms pair?) (tms a) (tms (list a)))

with the invariant that if (query (tms pair?)) is false, I promise not
to look in the payload of the tuple.  In particular, if all entries in
such a (tms pair?) are #f, no (tms a) or (tms (list a)) need be
stored, or empty such TMSes can be stored, and there is no infinite
regress.  Passing such a thing through some switch bodily will cause
the justification of the control to be attached to all the pieces, as
it should.  Merging two such things will recursively merge the
interior TMSes, as it should.

Why must this be so?  The argument flows from the assertion that
(join f g) = (lambda (x) (join (f x) (g x)))
and the Chruch encoding of cons cells.
Consider for simplicity a unary box (like a cons, but with only one
slot).  In Haskell, the Church encoding of this has the type
box :: a ((a -> a) -> a).
So once you've built one, you have an object whose type is
(a -> a) -> a.  But as
TMS((a -> a) -> a) = (a -> a) -> TMS(a)
it must be OK to push the TMS inside the box.  This feels a little
wrong: we seem to have gained the certainty that it is, in fact, a
box.  But Haskell has no box? function!  The only way to get at the
fact that it's a box is through the type system; the Church-encoded
object must be a function without qualification, and the only thing
that can be done with a function is applying it.  In order to implement
box? we must extend the Church encoding, i.e. to make a structure
that has a slot for its payload *and* a tag telling you what
structure it is.  But then the TMS over that can be pushed into
both places: a TMS over the possible type tags, and TMSes over
all the individual possible payloads.
